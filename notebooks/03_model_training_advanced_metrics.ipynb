{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_state": "idle",
   "id": "20e9407d-a22b-4962-8843-58f188aea7ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T22:18:52.499579Z",
     "iopub.status.busy": "2025-10-23T22:18:52.499204Z",
     "iopub.status.idle": "2025-10-23T22:21:04.738029Z",
     "shell.execute_reply": "2025-10-23T22:21:04.736979Z",
     "shell.execute_reply.started": "2025-10-23T22:18:52.499543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " ENTRAÎNEMENT DU MODÈLE AVANCÉ + MÉTRIQUES\n",
      "======================================================================\n",
      "\n",
      " Device: cpu\n",
      "   ⚠️  Mode CPU (normal pour SageMaker notebook)\n",
      "\n",
      "======================================================================\n",
      " CHARGEMENT DES DONNÉES PRÉPARÉES\n",
      "======================================================================\n",
      " Train: (80000, 22)\n",
      " Test:  (20000, 22)\n",
      "\n",
      " Dataset info:\n",
      "   Users: 943\n",
      "   Items: 1682\n",
      "   Features: 21 (sans rating)\n",
      "\n",
      "======================================================================\n",
      " CRÉATION DU DATASET PYTORCH\n",
      "======================================================================\n",
      " 19 features additionnelles:\n",
      "    1. age_normalized\n",
      "    2. gender_M\n",
      "    3. gender_F\n",
      "    4. occupation_encoded\n",
      "    5. user_rating_count\n",
      "    6. user_avg_rating\n",
      "    7. user_rating_std\n",
      "    8. num_genres\n",
      "    9. item_rating_count\n",
      "   10. item_avg_rating\n",
      "   11. item_rating_std\n",
      "   12. item_popularity_log\n",
      "   13. year\n",
      "   14. month\n",
      "   15. day_of_week\n",
      "   16. hour\n",
      "   17. genre_match_score\n",
      "   18. rating_diff_user_avg\n",
      "   19. rating_diff_item_avg\n",
      "\n",
      " DataLoaders créés (batch_size=256)\n",
      "   Train batches: 313\n",
      "   Test batches:  79\n",
      "\n",
      "======================================================================\n",
      "  ARCHITECTURE DU MODÈLE\n",
      "======================================================================\n",
      " Modèle créé:\n",
      "   Embeddings: 128D\n",
      "   Hidden layers: [256, 128, 64]\n",
      "   Total parameters: 462,209\n",
      "\n",
      "======================================================================\n",
      " DÉFINITION DES MÉTRIQUES\n",
      "======================================================================\n",
      " Métriques définies:\n",
      "   1. RMSE (Root Mean Square Error)\n",
      "   2. MAE (Mean Absolute Error)\n",
      "   3. Precision@K (K=10)\n",
      "   4. Hit Rate (threshold=4.0)\n",
      "\n",
      "======================================================================\n",
      " CONFIGURATION DE L'ENTRAÎNEMENT\n",
      "======================================================================\n",
      " Configuration:\n",
      "   Loss: MSE\n",
      "   Optimizer: Adam (lr=0.001, weight_decay=1e-5)\n",
      "   Scheduler: ReduceLROnPlateau\n",
      "\n",
      "======================================================================\n",
      " DÉBUT DE L'ENTRAÎNEMENT\n",
      "======================================================================\n",
      "\n",
      " Entraînement sur 15 epochs\n",
      "\n",
      "Epoch [ 1/15] | Train Loss: 5.4620 | Test Loss: 1.3550 | RMSE: 1.1660 | MAE: 0.9696 | Hit Rate: 0.443\n",
      "    Meilleur modèle sauvegardé (RMSE: 1.1660)\n",
      "Epoch [ 2/15] | Train Loss: 1.3495 | Test Loss: 1.1379 | RMSE: 1.0688 | MAE: 0.8705 | Hit Rate: 0.487\n",
      "    Meilleur modèle sauvegardé (RMSE: 1.0688)\n",
      "Epoch [ 3/15] | Train Loss: 0.9349 | Test Loss: 1.0903 | RMSE: 1.0462 | MAE: 0.8724 | Hit Rate: 0.490\n",
      "    Meilleur modèle sauvegardé (RMSE: 1.0462)\n",
      "Epoch [ 4/15] | Train Loss: 0.6940 | Test Loss: 0.4077 | RMSE: 0.6406 | MAE: 0.4811 | Hit Rate: 0.759\n",
      "    Meilleur modèle sauvegardé (RMSE: 0.6406)\n",
      "Epoch [ 5/15] | Train Loss: 0.6414 | Test Loss: 0.4380 | RMSE: 0.6641 | MAE: 0.5185 | Hit Rate: 0.701\n",
      "Epoch [ 6/15] | Train Loss: 0.5998 | Test Loss: 0.5048 | RMSE: 0.7130 | MAE: 0.5796 | Hit Rate: 0.656\n",
      "Epoch [ 7/15] | Train Loss: 0.5794 | Test Loss: 0.5909 | RMSE: 0.7714 | MAE: 0.6317 | Hit Rate: 0.641\n",
      "Epoch [ 8/15] | Train Loss: 0.5623 | Test Loss: 0.3983 | RMSE: 0.6334 | MAE: 0.4728 | Hit Rate: 0.728\n",
      "    Meilleur modèle sauvegardé (RMSE: 0.6334)\n",
      "Epoch [ 9/15] | Train Loss: 0.5380 | Test Loss: 0.3875 | RMSE: 0.6247 | MAE: 0.4492 | Hit Rate: 0.752\n",
      "    Meilleur modèle sauvegardé (RMSE: 0.6247)\n",
      "Epoch [10/15] | Train Loss: 0.5219 | Test Loss: 0.4510 | RMSE: 0.6742 | MAE: 0.5238 | Hit Rate: 0.675\n",
      "Epoch [11/15] | Train Loss: 0.5108 | Test Loss: 0.5679 | RMSE: 0.7562 | MAE: 0.6080 | Hit Rate: 0.661\n",
      "Epoch [12/15] | Train Loss: 0.5010 | Test Loss: 0.4350 | RMSE: 0.6622 | MAE: 0.4913 | Hit Rate: 0.697\n",
      "Epoch [13/15] | Train Loss: 0.4938 | Test Loss: 0.4967 | RMSE: 0.7076 | MAE: 0.5486 | Hit Rate: 0.678\n",
      "Epoch [14/15] | Train Loss: 0.4730 | Test Loss: 0.4203 | RMSE: 0.6508 | MAE: 0.4856 | Hit Rate: 0.694\n",
      "Epoch [15/15] | Train Loss: 0.4628 | Test Loss: 0.4286 | RMSE: 0.6572 | MAE: 0.4809 | Hit Rate: 0.704\n",
      "\n",
      "======================================================================\n",
      " ENTRAÎNEMENT TERMINÉ\n",
      "======================================================================\n",
      "\n",
      " Meilleur RMSE: 0.6247\n",
      "\n",
      "======================================================================\n",
      " GÉNÉRATION DES GRAPHIQUES\n",
      "======================================================================\n",
      " Graphique sauvegardé: outputs/plots/04_training_metrics.png\n",
      " Graphique sauvegardé: outputs/plots/05_prediction_analysis.png\n",
      "\n",
      "======================================================================\n",
      " GÉNÉRATION DU RAPPORT FINAL\n",
      "======================================================================\n",
      " Rapport JSON sauvegardé: outputs/metrics/training_report.json\n",
      "\n",
      "======================================================================\n",
      " RÉSULTATS FINAUX\n",
      "======================================================================\n",
      "\n",
      " Performances du modèle:\n",
      "    Meilleur RMSE: 0.6247\n",
      "    MAE final:     0.4809\n",
      "    Hit Rate:      0.704\n",
      "\n",
      " Fichiers sauvegardés:\n",
      "    Modèle: models/saved_models/best_model.pth\n",
      "    Graphiques: outputs/plots/\n",
      "    Métriques: outputs/metrics/training_report.json\n",
      "\n",
      " PROCHAINE ÉTAPE: Système de Recommandation Top-K\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=====================================================================\n",
    "NOTEBOOK 3 : MODÈLE AVANCÉ + MÉTRIQUES \n",
    "Projet : Système de Recommandation MovieLens sur Amazon SageMaker\n",
    "Auteur : Gninninmaguignon Silué\n",
    "Date : Octobre 2025\n",
    "=====================================================================\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" ENTRAÎNEMENT DU MODÈLE AVANCÉ + MÉTRIQUES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Vérifier GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"   ⚠️  Mode CPU (normal pour SageMaker notebook)\")\n",
    "\n",
    "# ============================================\n",
    "# PARTIE 1 : CHARGEMENT DES DONNÉES\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" CHARGEMENT DES DONNÉES PRÉPARÉES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "train_df = pd.read_csv(\"../data/processed/train_compact.csv\")\n",
    "test_df = pd.read_csv(\"../data/processed/test_compact.csv\")\n",
    "\n",
    "print(f\" Train: {train_df.shape}\")\n",
    "print(f\" Test:  {test_df.shape}\")\n",
    "\n",
    "# Nombre d'utilisateurs et de films\n",
    "n_users = train_df['user'].max() + 1\n",
    "n_items = train_df['item'].max() + 1\n",
    "print(f\"\\n Dataset info:\")\n",
    "print(f\"   Users: {n_users}\")\n",
    "print(f\"   Items: {n_items}\")\n",
    "print(f\"   Features: {train_df.shape[1] - 1} (sans rating)\")\n",
    "\n",
    "# ============================================\n",
    "# PARTIE 2 : DATASET PYTORCH\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" CRÉATION DU DATASET PYTORCH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class MovieLensDataset(Dataset):\n",
    "    \"\"\"Dataset PyTorch avec features complètes\"\"\"\n",
    "    \n",
    "    def __init__(self, df, feature_cols):\n",
    "        self.user = torch.tensor(df['user'].values, dtype=torch.long)\n",
    "        self.item = torch.tensor(df['item'].values, dtype=torch.long)\n",
    "        \n",
    "        # Features additionnelles (toutes sauf user, item, rating)\n",
    "        self.features = torch.tensor(\n",
    "            df[feature_cols].values, \n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        self.rating = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.rating)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.user[idx], self.item[idx], \n",
    "                self.features[idx], self.rating[idx])\n",
    "\n",
    "# Colonnes de features (sans user, item, rating)\n",
    "feature_cols = [col for col in train_df.columns \n",
    "                if col not in ['user', 'item', 'rating']]\n",
    "\n",
    "print(f\" {len(feature_cols)} features additionnelles:\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"   {i:2d}. {col}\")\n",
    "\n",
    "# Créer les datasets\n",
    "train_dataset = MovieLensDataset(train_df, feature_cols)\n",
    "test_dataset = MovieLensDataset(test_df, feature_cols)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                         shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                        shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"\\n DataLoaders créés (batch_size={batch_size})\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Test batches:  {len(test_loader)}\")\n",
    "\n",
    "# ============================================\n",
    "# PARTIE 3 : MODÈLE NEURAL HYBRID RECOMMENDER\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"  ARCHITECTURE DU MODÈLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class HybridRecommenderNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Modèle Hybrid: Collaborative Filtering + Content-Based\n",
    "    - Embeddings pour user/item\n",
    "    - Features additionnelles\n",
    "    - Architecture deep avec dropout et batch norm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_users, n_items, n_features, \n",
    "                 embedding_dim=128, hidden_dims=[256, 128, 64]):\n",
    "        super(HybridRecommenderNet, self).__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        \n",
    "        # Batch Normalization pour embeddings\n",
    "        self.user_bn = nn.BatchNorm1d(embedding_dim)\n",
    "        self.item_bn = nn.BatchNorm1d(embedding_dim)\n",
    "        \n",
    "        # Réseau pour features additionnelles\n",
    "        self.feature_fc = nn.Sequential(\n",
    "            nn.Linear(n_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "        \n",
    "        # Réseau principal (concat embeddings + features)\n",
    "        total_input = embedding_dim * 2 + 64\n",
    "        \n",
    "        layers = []\n",
    "        input_dim = total_input\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.BatchNorm1d(hidden_dim)\n",
    "            ])\n",
    "            input_dim = hidden_dim\n",
    "        \n",
    "        # Couche de sortie\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialisation Xavier\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialisation des poids\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "    \n",
    "    def forward(self, user, item, features):\n",
    "        # Embeddings\n",
    "        user_emb = self.user_embedding(user)\n",
    "        item_emb = self.item_embedding(item)\n",
    "        \n",
    "        user_emb = self.user_bn(user_emb)\n",
    "        item_emb = self.item_bn(item_emb)\n",
    "        \n",
    "        # Features\n",
    "        feat_emb = self.feature_fc(features)\n",
    "        \n",
    "        # Concatenation\n",
    "        x = torch.cat([user_emb, item_emb, feat_emb], dim=1)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = self.fc_layers(x)\n",
    "        \n",
    "        return output.squeeze()\n",
    "\n",
    "# Créer le modèle\n",
    "n_features = len(feature_cols)\n",
    "model = HybridRecommenderNet(\n",
    "    n_users=n_users,\n",
    "    n_items=n_items,\n",
    "    n_features=n_features,\n",
    "    embedding_dim=128,\n",
    "    hidden_dims=[256, 128, 64]\n",
    ").to(device)\n",
    "\n",
    "print(\" Modèle créé:\")\n",
    "print(f\"   Embeddings: {128}D\")\n",
    "print(f\"   Hidden layers: [256, 128, 64]\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# ============================================\n",
    "# PARTIE 4 : FONCTIONS DE MÉTRIQUES\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" DÉFINITION DES MÉTRIQUES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def calculate_rmse(predictions, targets):\n",
    "    \"\"\"Root Mean Square Error\"\"\"\n",
    "    return np.sqrt(mean_squared_error(targets, predictions))\n",
    "\n",
    "def calculate_mae(predictions, targets):\n",
    "    \"\"\"Mean Absolute Error\"\"\"\n",
    "    return mean_absolute_error(targets, predictions)\n",
    "\n",
    "def calculate_precision_at_k(model, user_item_matrix, k=10):\n",
    "    \"\"\"\n",
    "    Precision@K: Proportion de films recommandés qui sont pertinents\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for user_id in range(min(100, n_users)):  # Échantillon\n",
    "            # Films déjà notés par l'utilisateur\n",
    "            rated_items = user_item_matrix[user_id].nonzero()[0]\n",
    "            \n",
    "            if len(rated_items) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Prédire pour tous les films\n",
    "            user_tensor = torch.tensor([user_id] * n_items, \n",
    "                                      dtype=torch.long).to(device)\n",
    "            item_tensor = torch.arange(n_items, \n",
    "                                      dtype=torch.long).to(device)\n",
    "            \n",
    "            # Features moyennes (simplification pour la démo)\n",
    "            feat_mean = torch.zeros(n_items, n_features).to(device)\n",
    "            \n",
    "            predictions = model(user_tensor, item_tensor, feat_mean).cpu().numpy()\n",
    "            \n",
    "            # Top-K recommandations\n",
    "            top_k_items = np.argsort(predictions)[-k:]\n",
    "            \n",
    "            # Films pertinents (rating >= 4)\n",
    "            relevant_items = set([i for i in rated_items \n",
    "                                if user_item_matrix[user_id, i] >= 4])\n",
    "            \n",
    "            # Precision\n",
    "            if len(relevant_items) > 0:\n",
    "                hits = len(set(top_k_items) & relevant_items)\n",
    "                precisions.append(hits / k)\n",
    "    \n",
    "    return np.mean(precisions) if precisions else 0.0\n",
    "\n",
    "def calculate_hit_rate(predictions, targets, threshold=4.0):\n",
    "    \"\"\"Hit Rate: Proportion de prédictions correctes (rating >= threshold)\"\"\"\n",
    "    pred_binary = (predictions >= threshold).astype(int)\n",
    "    target_binary = (targets >= threshold).astype(int)\n",
    "    return np.mean(pred_binary == target_binary)\n",
    "\n",
    "print(\" Métriques définies:\")\n",
    "print(\"   1. RMSE (Root Mean Square Error)\")\n",
    "print(\"   2. MAE (Mean Absolute Error)\")\n",
    "print(\"   3. Precision@K (K=10)\")\n",
    "print(\"   4. Hit Rate (threshold=4.0)\")\n",
    "\n",
    "# ============================================\n",
    "# PARTIE 5 : FONCTION D'ENTRAÎNEMENT\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" CONFIGURATION DE L'ENTRAÎNEMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "print(\" Configuration:\")\n",
    "print(f\"   Loss: MSE\")\n",
    "print(f\"   Optimizer: Adam (lr=0.001, weight_decay=1e-5)\")\n",
    "print(f\"   Scheduler: ReduceLROnPlateau\")\n",
    "\n",
    "# ============================================\n",
    "# PARTIE 6 : BOUCLE D'ENTRAÎNEMENT\n",
    "# ============================================\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Entraîner une epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for users, items, features, ratings in loader:\n",
    "        users = users.to(device)\n",
    "        items = items.to(device)\n",
    "        features = features.to(device)\n",
    "        ratings = ratings.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(users, items, features)\n",
    "        loss = criterion(predictions, ratings)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Évaluer le modèle\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for users, items, features, ratings in loader:\n",
    "            users = users.to(device)\n",
    "            items = items.to(device)\n",
    "            features = features.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "            \n",
    "            predictions = model(users, items, features)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(ratings.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    # Calculer les métriques\n",
    "    rmse = calculate_rmse(all_predictions, all_targets)\n",
    "    mae = calculate_mae(all_predictions, all_targets)\n",
    "    hit_rate = calculate_hit_rate(all_predictions, all_targets)\n",
    "    \n",
    "    return avg_loss, rmse, mae, hit_rate, all_predictions, all_targets\n",
    "\n",
    "# ============================================\n",
    "# PARTIE 7 : ENTRAÎNEMENT\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" DÉBUT DE L'ENTRAÎNEMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "n_epochs = 15\n",
    "best_rmse = float('inf')\n",
    "history = {\n",
    "    'train_loss': [], 'test_loss': [],\n",
    "    'test_rmse': [], 'test_mae': [], 'test_hit_rate': []\n",
    "}\n",
    "\n",
    "print(f\"\\n Entraînement sur {n_epochs} epochs\\n\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Entraînement\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Évaluation\n",
    "    test_loss, rmse, mae, hit_rate, _, _ = evaluate(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Enregistrer\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_rmse'].append(rmse)\n",
    "    history['test_mae'].append(mae)\n",
    "    history['test_hit_rate'].append(hit_rate)\n",
    "    \n",
    "    # Scheduler\n",
    "    scheduler.step(test_loss)\n",
    "    \n",
    "    # Affichage\n",
    "    print(f\"Epoch [{epoch+1:2d}/{n_epochs}] | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Test Loss: {test_loss:.4f} | \"\n",
    "          f\"RMSE: {rmse:.4f} | \"\n",
    "          f\"MAE: {mae:.4f} | \"\n",
    "          f\"Hit Rate: {hit_rate:.3f}\")\n",
    "    \n",
    "    # Sauvegarder le meilleur modèle\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'n_users': n_users,\n",
    "            'n_items': n_items,\n",
    "            'n_features': n_features,\n",
    "            'feature_cols': feature_cols\n",
    "        }, '../models/saved_models/best_model.pth')\n",
    "        print(f\"    Meilleur modèle sauvegardé (RMSE: {rmse:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" ENTRAÎNEMENT TERMINÉ\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n Meilleur RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# PARTIE 8 : VISUALISATION DES RÉSULTATS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" GÉNÉRATION DES GRAPHIQUES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Résultats d\\'Entraînement - Modèle Hybrid', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "epochs_range = range(1, n_epochs + 1)\n",
    "\n",
    "# 1. Losses\n",
    "ax = axes[0, 0]\n",
    "ax.plot(epochs_range, history['train_loss'], \n",
    "        marker='o', label='Train Loss', linewidth=2)\n",
    "ax.plot(epochs_range, history['test_loss'], \n",
    "        marker='s', label='Test Loss', linewidth=2)\n",
    "ax.set_title('Evolution des Losses', fontweight='bold')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. RMSE\n",
    "ax = axes[0, 1]\n",
    "ax.plot(epochs_range, history['test_rmse'], \n",
    "        marker='o', color='coral', linewidth=2)\n",
    "ax.set_title('Evolution du RMSE', fontweight='bold')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. MAE\n",
    "ax = axes[1, 0]\n",
    "ax.plot(epochs_range, history['test_mae'], \n",
    "        marker='s', color='mediumseagreen', linewidth=2)\n",
    "ax.set_title('Evolution du MAE', fontweight='bold')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MAE')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Hit Rate\n",
    "ax = axes[1, 1]\n",
    "ax.plot(epochs_range, history['test_hit_rate'], \n",
    "        marker='^', color='mediumpurple', linewidth=2)\n",
    "ax.set_title('Evolution du Hit Rate', fontweight='bold')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Hit Rate')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/plots/04_training_metrics.png', \n",
    "            dpi=150, bbox_inches='tight')\n",
    "print(\" Graphique sauvegardé: outputs/plots/04_training_metrics.png\")\n",
    "plt.close()\n",
    "\n",
    "# Distribution des erreurs\n",
    "_, _, _, _, final_preds, final_targets = evaluate(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "errors = final_preds - final_targets\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogramme des erreurs\n",
    "axes[0].hist(errors, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(0, color='red', linestyle='--', linewidth=2, label='Erreur = 0')\n",
    "axes[0].set_title('Distribution des Erreurs de Prédiction', fontweight='bold')\n",
    "axes[0].set_xlabel('Erreur (Prédiction - Réel)')\n",
    "axes[0].set_ylabel('Fréquence')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot: Prédictions vs Réel\n",
    "axes[1].scatter(final_targets, final_preds, alpha=0.3, s=10)\n",
    "axes[1].plot([1, 5], [1, 5], 'r--', linewidth=2, label='Prédiction parfaite')\n",
    "axes[1].set_title('Prédictions vs Ratings Réels', fontweight='bold')\n",
    "axes[1].set_xlabel('Rating Réel')\n",
    "axes[1].set_ylabel('Rating Prédit')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/plots/05_prediction_analysis.png', \n",
    "            dpi=150, bbox_inches='tight')\n",
    "print(\" Graphique sauvegardé: outputs/plots/05_prediction_analysis.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================\n",
    "# PARTIE 9 : RAPPORT FINAL\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" GÉNÉRATION DU RAPPORT FINAL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "final_metrics = {\n",
    "    'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'model': 'HybridRecommenderNet',\n",
    "    'architecture': {\n",
    "        'embedding_dim': 128,\n",
    "        'hidden_dims': [256, 128, 64],\n",
    "        'total_parameters': sum(p.numel() for p in model.parameters()),\n",
    "        'n_features': n_features\n",
    "    },\n",
    "    'training': {\n",
    "        'epochs': n_epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'optimizer': 'Adam',\n",
    "        'learning_rate': 0.001,\n",
    "        'scheduler': 'ReduceLROnPlateau'\n",
    "    },\n",
    "    'final_metrics': {\n",
    "        'best_rmse': float(best_rmse),\n",
    "        'final_rmse': float(history['test_rmse'][-1]),\n",
    "        'final_mae': float(history['test_mae'][-1]),\n",
    "        'final_hit_rate': float(history['test_hit_rate'][-1]),\n",
    "        'train_loss': float(history['train_loss'][-1]),\n",
    "        'test_loss': float(history['test_loss'][-1])\n",
    "    },\n",
    "    'history': {\n",
    "        'test_rmse': [float(x) for x in history['test_rmse']],\n",
    "        'test_mae': [float(x) for x in history['test_mae']],\n",
    "        'test_hit_rate': [float(x) for x in history['test_hit_rate']]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../outputs/metrics/training_report.json', 'w') as f:\n",
    "    json.dump(final_metrics, f, indent=2)\n",
    "print(\" Rapport JSON sauvegardé: outputs/metrics/training_report.json\")\n",
    "\n",
    "# ============================================\n",
    "# RÉSUMÉ FINAL\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" RÉSULTATS FINAUX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n Performances du modèle:\")\n",
    "print(f\"    Meilleur RMSE: {best_rmse:.4f}\")\n",
    "print(f\"    MAE final:     {history['test_mae'][-1]:.4f}\")\n",
    "print(f\"    Hit Rate:      {history['test_hit_rate'][-1]:.3f}\")\n",
    "\n",
    "print(f\"\\n Fichiers sauvegardés:\")\n",
    "print(f\"    Modèle: models/saved_models/best_model.pth\")\n",
    "print(f\"    Graphiques: outputs/plots/\")\n",
    "print(f\"    Métriques: outputs/metrics/training_report.json\")\n",
    "\n",
    "print(\"\\n PROCHAINE ÉTAPE: Système de Recommandation Top-K\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
